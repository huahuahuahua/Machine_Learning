---
title: "machine_learning_course_project"
author: "H.S."
date: "December 24, 2017"
output: html_document
---

# Course Project 

The goal of your project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. You may use any of the other variables to predict with. You should create a report describing how you built your model, how you used cross validation, what you think the expected out of sample error is, and why you made the choices you did. You will also use your prediction model to predict 20 different test cases. 

## Course Project Prediction Quiz Portion  

Apply your machine learning algorithm to the 20 test cases available in the test data above and submit your predictions in appropriate format to the Course Project Prediction Quiz for automated grading.   


Data  

The training data for this project are available here:  
<https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv>  
The test data are available here:  
<https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv>  
The data for this project come from this source: <http://groupware.les.inf.puc-rio.br/har>.  

```{r load library}
library(ggplot2)
library(caret)
library(randomForest)
library(rattle)
```
```{r load training and testing data}
trainP <- read.csv("pml-training.csv", na.strings = c("NA", ""))
testingP <-read.csv("pml-testing.csv", na.strings = c("NA", ""))
dim(trainP); dim(testingP)
```
## Data cleaning includes 3 steps as below:  
* Remove columns with NA values.  
. function colSum and is.na are used to capture NA columns and exclude them from the data set.  
* Remove columns with no variations.  
. function nearZeroVar is used to identify this type of columns.
* Remove columns with no relationship with the prediction.  
. based on my understanding of the data columns "X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "num_window" are not related to the prediction and so are removed from the data set.  

```{r clean up training data}
# columns with NA count > 0 will be captured in dataNA
dataNA <- colSums(is.na(trainP)) > 0 

# remove the NA columns from training set
trainP1 <- trainP[, !dataNA]
#Remove columns with no variations
NZV <- nearZeroVar(trainP1, saveMetrics = TRUE)
trainP2 <- trainP1[, !NZV$nzv]
# Remove columns with no relationship with the prediction
trainP3 <- trainP2[, -c(1, 2, 3, 4, 5, 6)]
```
## Data Slicing
split the training data into training set and validation set.  

```{r data slicing}
inTrain <- createDataPartition(y=trainP3$classe, p=0.7, list=FALSE)
validation <- trainP3[-inTrain,]
training <- trainP3[inTrain,]
dim(training)
dim(validation)
```
## Modeling  
The training set we are using has a lot of data, many variables, and the possibility of noise in the data. Random forest may be the best method to use.
```{r}
tc <- trainControl(method = "cv", 3)
modFitP <- train(classe ~., method = "rf", data = training, trControl = tc, allowParallel=TRUE, importance=TRUE, ntree = 250 )
#modFitP <- randomForest(classe ~., data = training)

modFitP
```
```{r predict with validation data}
prdval <- predict(modFitP, validation)
confusionMatrix(validation$classe, prdval)
```
```{r}
accuracy <- postResample(prdval, validation$classe)
outsamperr <- 1 - as.numeric(confusionMatrix(validation$classe, prdval)$overall[1])
print(accuracy); print(outsamperr)
```
* As we can see from this execution the accuracy is 99.39%  
* Out of Sample Error rate is 0.61%

## Variable Importance  
```{r}
varImpt <- varImp(modFitP)$importance
head(varImpt)
```

## Plotting the accuracy  


```{r}
qplot(classe, prdval, data=validation,  colour= classe, geom = c("boxplot", "jitter"), main = "predicted vs. observed in validation data", xlab = "Observed Classe", ylab = "Predicted Classe")

```

## Apply model to testing set  
```{r}
prdvalT <- predict(modFitP, testingP)
#confusionMatrix(testingP$classe, prdvalT)
prdvalT
```